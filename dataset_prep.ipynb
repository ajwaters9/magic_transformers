{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ztbzZheqRGgL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "import os\n",
        "import re\n",
        "# from google.colab import files\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4zrKVV3HpuP"
      },
      "source": [
        "## Download card-level data from 17lands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGRH6QRzHsk-",
        "outputId": "73b0842d-9a44-4ef6-b4d8-1d37bfa3c407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-22 10:47:22--  https://17lands-public.s3.amazonaws.com/analysis_data/cards/cards.csv\n",
            "Resolving 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)... 54.231.233.41, 3.5.16.146, 52.216.28.100, ...\n",
            "Connecting to 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)|54.231.233.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1039892 (1016K) [text/csv]\n",
            "Saving to: ‘cards.csv’\n",
            "\n",
            "cards.csv           100%[===================>]   1016K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-22 10:47:23 (8.30 MB/s) - ‘cards.csv’ saved [1039892/1039892]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://17lands-public.s3.amazonaws.com/analysis_data/cards/cards.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwIlbd8Hhan"
      },
      "source": [
        "## Get dtypes for draft data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wmrpeSWrN46O"
      },
      "outputs": [],
      "source": [
        "# Helper function provided by 17lands.com to set proper datatypes for draft data files\n",
        "\n",
        "def get_dtypes(filename: str, print_missing: bool = False) -> Dict[str, str]:\n",
        "    dtypes: Dict[str, str] = {}\n",
        "    for column in pd.read_csv(filename, nrows=0).columns:\n",
        "        for regex, column_type in COLUMN_TYPES:\n",
        "            if regex.match(column):\n",
        "                dtypes[column] = column_type\n",
        "                break\n",
        "        else:\n",
        "            if print_missing:\n",
        "                print(f\"Could not find an appropriate type for {column}\")\n",
        "    return dtypes\n",
        "\n",
        "COLUMN_TYPES = (\n",
        "    # Metadata\n",
        "    (re.compile(r\"^user_n_games_bucket$\"), \"int16\"),\n",
        "    (re.compile(r\"^user_game_win_rate_bucket$\"), \"float\"),\n",
        "    (re.compile(r\"^expansion$\"), \"str\"),\n",
        "    (re.compile(r\"^event_type$\"), \"str\"),\n",
        "    (re.compile(r\"^draft_id$\"), \"str\"),\n",
        "    (re.compile(r\"^draft_time$\"), \"str\"),\n",
        "    (re.compile(r\"^rank$\"), \"str\"),\n",
        "    # Draft\n",
        "    (re.compile(r\"^event_match_wins$\"), \"int8\"),\n",
        "    (re.compile(r\"^event_match_losses$\"), \"int8\"),\n",
        "    (re.compile(r\"^pack_number$\"), \"int8\"),\n",
        "    (re.compile(r\"^pick_number$\"), \"int8\"),\n",
        "    (re.compile(r\"^pick$\"), \"str\"),\n",
        "    (re.compile(r\"^pick_maindeck_rate$\"), \"float\"),\n",
        "    (re.compile(r\"^pick_sideboard_in_rate$\"), \"float\"),\n",
        "    (re.compile(r\"^pool_.*\"), \"int8\"),\n",
        "    (re.compile(r\"^pack_card_.*\"), \"int8\"),\n",
        "    # Game + Replay\n",
        "    (re.compile(r\"^game_time$\"), \"str\"),\n",
        "    (re.compile(r\"^build_index$\"), \"int8\"),\n",
        "    (re.compile(r\"^match_number$\"), \"int8\"),\n",
        "    (re.compile(r\"^game_number$\"), \"int8\"),\n",
        "    (re.compile(r\"^opp_rank$\"), \"str\"),\n",
        "    (re.compile(r\"^main_colors$\"), \"str\"),\n",
        "    (re.compile(r\"^splash_colors$\"), \"str\"),\n",
        "    (re.compile(r\"^on_play$\"), \"bool\"),\n",
        "    (re.compile(r\"^num_mulligans$\"), \"int8\"),\n",
        "    (re.compile(r\"^opp_num_mulligans$\"), \"int8\"),\n",
        "    (re.compile(r\"^opp_colors$\"), \"str\"),\n",
        "    (re.compile(r\"^num_turns$\"), \"int8\"),\n",
        "    (re.compile(r\"^won$\"), \"bool\"),\n",
        "    (re.compile(r\"^deck_.*\"), \"int8\"),\n",
        "    (re.compile(r\"^sideboard_.*\"), \"int8\"),\n",
        "    # Game\n",
        "    (re.compile(r\"^drawn_.*\"), \"int8\"),\n",
        "    (re.compile(r\"^tutored_.*\"), \"int8\"),\n",
        "    (re.compile(r\"^opening_hand_.*\"), \"int8\"),\n",
        "    # Replay\n",
        "    (re.compile(r\"^candidate_hand_\\d$\"), \"str\"),\n",
        "    (re.compile(r\"^opening_hand$\"), \"str\"),\n",
        "    (re.compile(r\"^user_turn_\\d+_cards_drawn$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_cards_discarded$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_lands_played$\"), \"str\"),\n",
        "    (re.compile(r\"^user_turn_\\d+_cards_foretold$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_cast$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_non_creatures_cast$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_instants_sorceries_cast$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_abilities$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_cards_learned$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_attacked$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_blocked$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_unblocked$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_blocking$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_creatures_blitzed$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_player_combat_damage_dealt$\"), \"str\"),  # DEPRECATED\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_combat_damage_taken$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_creatures_killed_combat$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_creatures_killed_non_combat$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_((user)|(oppo))_mana_spent$\"), \"float\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_user_cards_in_hand$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_oppo_cards_in_hand$\"), \"float\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_((user)|(oppo))_lands_in_play$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_((user)|(oppo))_creatures_in_play$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_((user)|(oppo))_non_creatures_in_play$\"), \"str\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_((user)|(oppo))_life$\"), \"float\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_turn_\\d+_eot_((user)|(oppo))_poison_counters$\"), \"float\"),\n",
        "    (re.compile(r\"^user_turn_\\d+_cards_tutored$\"), \"str\"),\n",
        "    (re.compile(r\"^oppo_turn_\\d+_cards_tutored$\"), \"int8\"),\n",
        "    (re.compile(r\"^oppo_turn_\\d+_cards_drawn_or_tutored$\"), \"int8\"),\n",
        "    (re.compile(r\"^oppo_turn_\\d+_cards_drawn$\"), \"int8\"),\n",
        "    (re.compile(r\"^oppo_turn_\\d+_cards_foretold$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_cards_drawn$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_cards_discarded$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_lands_played$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_cards_foretold$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_creatures_cast$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_creatures_blitzed$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_non_creatures_cast$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_instants_sorceries_cast$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_cards_learned$\"), \"int8\"),\n",
        "    (re.compile(r\"^((user)|(oppo))_total_mana_spent$\"), \"int16\"),\n",
        "    (re.compile(r\"^oppo_total_cards_drawn_or_tutored$\"), \"int8\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gs6eRQBHXOy"
      },
      "source": [
        "## Function to validate Wins/Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8iPwYIZRPwx5"
      },
      "outputs": [],
      "source": [
        "# Function to validate Wins/Losses in Draft Data with accurate values from Gameplay Data\n",
        "\n",
        "def get_game_wins(set_name):\n",
        "    # Load gameplay results data\n",
        "    filename = f'game_data_public.{set_name}.PremierDraft.csv.gz'\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        webpath = f'https://17lands-public.s3.amazonaws.com/analysis_data/game_data/game_data_public.{set_name}.PremierDraft.csv.gz'\n",
        "        !wget {webpath}\n",
        "        print(f\"Downloaded {filename}.\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists, no need to download.\")\n",
        "\n",
        "    datatypes = get_dtypes(filename)\n",
        "    game_df = pd.read_csv(filename, usecols=[2,16,17], dtype=datatypes, compression='gzip')\n",
        "\n",
        "    # Get a count of number of games played\n",
        "    game_df_2 = game_df.groupby('draft_id').count().reset_index()\n",
        "    # Get the sum of games won per draft_id\n",
        "    game_wins = game_df.groupby('draft_id').sum().reset_index()['won']\n",
        "\n",
        "    # Populate dataframe with corrected values for wins and losses\n",
        "    game_df_2.rename(columns={'won':'games'}, inplace=True)\n",
        "    game_df_2['wins'] = game_wins\n",
        "    game_df_2['wins'] = game_df_2['wins'].clip(upper=7)  # Some MKM drafts logged 8 wins\n",
        "\n",
        "    # Create dictionary of draft ID and corrected value of wins\n",
        "    game_wins_dict = game_df_2.set_index('draft_id')['wins'].to_dict()\n",
        "    return game_wins_dict\n",
        "\n",
        "\n",
        "def map_game_wins(df, game_wins_dict):\n",
        "    # Map corrected event wins to draft dataframe\n",
        "    df['wins'] = df['draft_id'].map(game_wins_dict)\n",
        "    # Drop rows where no games were ever recorded\n",
        "    drop_rows = df.loc[df['wins'].isna()].index\n",
        "    df.drop(drop_rows, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df['wins'] = df['wins'].astype('int')\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd6UI6vxERJ5"
      },
      "source": [
        "## Define Class to handle data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JAKlMe7sn-7E"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataPreprocess:\n",
        "\n",
        "    def __init__(self, df, set_name):\n",
        "        self.set_name = set_name\n",
        "        self.df = df\n",
        "        self.ordered_data = None\n",
        "        self.max_pack_length = df['pick_number'].max() + 1\n",
        "        self.pack_cards_indices = []\n",
        "        self.pool_cards_indices = []\n",
        "        self.set_cards = None\n",
        "        self.vocab_size = None\n",
        "\n",
        "    def initialize(self):\n",
        "        self.calculate_indices()\n",
        "        self.load_set_cards()\n",
        "\n",
        "    def calculate_indices(self):\n",
        "        for i, col in enumerate(self.df.columns):\n",
        "            if col.startswith('pack_card_'):\n",
        "                self.pack_cards_indices.append(i)\n",
        "            elif col.startswith('pool_'):\n",
        "                self.pool_cards_indices.append(i)\n",
        "\n",
        "    def load_set_cards(self):\n",
        "        cards_df = pd.read_csv('cards_list.csv')\n",
        "        df_pack_subset = self.df.iloc[:, self.pack_cards_indices]\n",
        "        df_pack_subset.columns = df_pack_subset.columns.str.replace('pack_card_', '')\n",
        "        card_names = df_pack_subset.columns.tolist()\n",
        "        cards_df = cards_df[cards_df['name'].isin(card_names)]\n",
        "        cards_df = cards_df.drop_duplicates(subset='name').sort_values(by='id').reset_index(drop=True)\n",
        "        cards_df['token_id'] = cards_df.index + 2\n",
        "        self.set_cards = cards_df[['name', 'token_id', 'id']]\n",
        "        self.vocab_size = self.set_cards['token_id'].max()\n",
        "\n",
        "    def process_data(self):\n",
        "        print('Preprocessing Data...')\n",
        "        self.ordered_data = self.order_draft_data()\n",
        "        self.handle_pick_sequences()\n",
        "        self.handle_pack_cards()\n",
        "        # self.apply_padding()\n",
        "        self.finalize_data()\n",
        "        self.collate_data()\n",
        "        print('Preprocessing Complete')\n",
        "\n",
        "    def order_draft_data(self):\n",
        "        card_mapper = {row.name: row.token_id for row in self.set_cards.itertuples()}\n",
        "        complete_drafts = self.df.groupby('draft_id').count()['event_match_wins'] == (self.max_pack_length * 3)\n",
        "        ordered_data = self.df[self.df['draft_id'].isin(complete_drafts.index)].sort_values(by=['draft_id', 'pack_number', 'pick_number'])\n",
        "        ordered_data['pick_state'] = ordered_data['pack_number'] * self.max_pack_length + ordered_data['pick_number']\n",
        "        ordered_data['pick_token'] = ordered_data['pick'].map(card_mapper)\n",
        "        return ordered_data\n",
        "\n",
        "    def handle_pick_sequences(self):\n",
        "        full_sequence = {id: group['pick_token'].to_numpy() for id, group in self.ordered_data.groupby('draft_id')}\n",
        "        self.ordered_data['pick_sequence'] = self.ordered_data['draft_id'].map(full_sequence)\n",
        "        self.ordered_data['current_pool'] = self.ordered_data.apply(lambda row: row['pick_sequence'][:row['pick_state']], axis=1)\n",
        "\n",
        "    def handle_pack_cards(self):\n",
        "        card_name_to_id = {row['name']: row['token_id'] for _, row in self.set_cards.iterrows()}\n",
        "        def extract_cards(row):\n",
        "            cards = [name.replace('pack_card_', '') for name, value in row[self.df.columns[self.pack_cards_indices]].items() if value != 0]\n",
        "            return [card_name_to_id.get(card, 0) for card in cards]\n",
        "        self.ordered_data['available_cards'] = self.ordered_data.apply(extract_cards, axis=1)\n",
        "\n",
        "    def apply_padding(self):\n",
        "        max_length = self.ordered_data['current_pool'].apply(len).max()\n",
        "        def pad_sequence(sequence):\n",
        "            sequence = list(sequence)\n",
        "            if len(sequence) == 0:\n",
        "                return [1] + [0] * (max_length - 1)\n",
        "            else:\n",
        "                return sequence + [0] * (max_length - len(sequence))\n",
        "        self.ordered_data['padded_pool'] = self.ordered_data['current_pool'].apply(pad_sequence)\n",
        "\n",
        "    def insert_win_token(self, sequence, wins):\n",
        "        a_len = sequence.shape[0]\n",
        "        new_a = np.full(a_len + 1, (self.vocab_size + 1 + wins))\n",
        "        new_a[1:] = sequence\n",
        "        return new_a\n",
        "\n",
        "    def finalize_data(self):\n",
        "        self.ordered_data = self.ordered_data[self.ordered_data['available_cards'].apply(len) > 1]\n",
        "        self.ordered_data['event_match_wins'] = self.ordered_data['event_match_wins'].astype('int')\n",
        "        self.ordered_data['current_pool_with_wins'] = self.ordered_data.apply(lambda x: self.insert_win_token(x['current_pool'], x['event_match_wins']), axis=1)\n",
        "        self.ordered_data.reset_index(drop=False, inplace=True)\n",
        "\n",
        "    # def collate_data(self):\n",
        "    #     self.df_cleaned = self.ordered_data[['draft_id','padded_pool', 'available_cards', 'pick_token', 'event_match_wins']].rename(columns={\n",
        "    #         'padded_pool': 'cards_in_pool',\n",
        "    #         'available_cards': 'cards_in_pack',\n",
        "    #         'pick_token': 'card_selected',\n",
        "    #         'event_match_wins': 'wins'\n",
        "    #     })\n",
        "    def collate_data(self):\n",
        "        self.df_cleaned = self.ordered_data[['index','draft_id','current_pool','current_pool_with_wins', 'available_cards', 'pick_token', 'event_match_wins']].rename(columns={\n",
        "            'current_pool': 'cards_in_pool',\n",
        "            'available_cards': 'cards_in_pack',\n",
        "            'pick_token': 'card_selected',\n",
        "            'event_match_wins': 'wins'\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNMPEVoDRAL"
      },
      "source": [
        "## Load raw draft data, or download directly from 17lands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "KZx6YQSaIstD"
      },
      "outputs": [],
      "source": [
        "# Specify the 3 letter set acronym\n",
        "\n",
        "SET_NAME = 'MKM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i412tUVDog2X",
        "outputId": "57489604-7c0a-4578-d638-c83dfc280135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-22 10:47:38--  https://17lands-public.s3.amazonaws.com/analysis_data/draft_data/draft_data_public.MKM.PremierDraft.csv.gz\n",
            "Resolving 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)... 54.231.165.137, 52.217.167.153, 16.182.38.177, ...\n",
            "Connecting to 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)|54.231.165.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 218519611 (208M) [text/csv]\n",
            "Saving to: ‘draft_data_public.MKM.PremierDraft.csv.gz’\n",
            "\n",
            "draft_data_public.M 100%[===================>] 208.40M  31.6MB/s    in 6.0s    \n",
            "\n",
            "2024-05-22 10:47:45 (34.6 MB/s) - ‘draft_data_public.MKM.PremierDraft.csv.gz’ saved [218519611/218519611]\n",
            "\n",
            "Downloaded draft_data_public.MKM.PremierDraft.csv.gz.\n",
            "--2024-05-22 10:47:45--  https://17lands-public.s3.amazonaws.com/analysis_data/game_data/game_data_public.MKM.PremierDraft.csv.gz\n",
            "Resolving 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)... 52.217.165.49, 16.182.97.41, 3.5.29.146, ...\n",
            "Connecting to 17lands-public.s3.amazonaws.com (17lands-public.s3.amazonaws.com)|52.217.165.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76206592 (73M) [text/csv]\n",
            "Saving to: ‘game_data_public.MKM.PremierDraft.csv.gz’\n",
            "\n",
            "game_data_public.MK 100%[===================>]  72.68M  39.3MB/s    in 1.9s    \n",
            "\n",
            "2024-05-22 10:47:47 (39.3 MB/s) - ‘game_data_public.MKM.PremierDraft.csv.gz’ saved [76206592/76206592]\n",
            "\n",
            "Downloaded game_data_public.MKM.PremierDraft.csv.gz.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "filename = f'draft_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "gameplay_filename = f'game_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "\n",
        "# Check if the file already exists, otherwise download it\n",
        "if not os.path.exists(filename):\n",
        "    webpath = f'https://17lands-public.s3.amazonaws.com/analysis_data/draft_data/draft_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "    !wget {webpath}\n",
        "    print(f\"Downloaded {filename}.\")\n",
        "else:\n",
        "    print(f\"{filename} already exists, no need to download.\")\n",
        "\n",
        "if not os.path.exists(gameplay_filename):\n",
        "    webpath = f'https://17lands-public.s3.amazonaws.com/analysis_data/game_data/game_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "    !wget {webpath}\n",
        "    print(f\"Downloaded {gameplay_filename}.\")\n",
        "else:\n",
        "    print(f\"{gameplay_filename} already exists, no need to download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB0PNtFceqou"
      },
      "source": [
        "## Prepare Card-Level data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "SET_NAME = 'SIR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qSTlVHFdeqov"
      },
      "outputs": [],
      "source": [
        "# Import prepared card feature and oracle text files\n",
        "card_features = pd.read_parquet('card_features_reduced.parquet')\n",
        "oracle_texts = pd.read_parquet('oracle_text.parquet')\n",
        "\n",
        "# Extract Set card list from DataPreprocessing Object\n",
        "# filename = f'draft_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "filename = 'draft_data_SIR\\SIR_premier_draft_1.csv'\n",
        "datatypes = get_dtypes(filename)\n",
        "data_header = pd.read_csv(filename, dtype=datatypes, nrows=0)\n",
        "card_head = DataPreprocess(data_header, set_name=SET_NAME)\n",
        "card_head.initialize()\n",
        "cards_df = card_head.set_cards\n",
        "\n",
        "# Merge Set card list with imported features\n",
        "cards_df = cards_df.merge(oracle_texts, left_on='name', right_on='name').drop(columns=['arena_id'], axis=1)\n",
        "cards_df = cards_df.merge(card_features, left_on='name', right_on='name')\n",
        "\n",
        "# # Export combined dataset to file\n",
        "# out_filename = f'{SET_NAME}_cards.parquet' \n",
        "# cards_df.to_parquet(out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNjaZvE5uRXV"
      },
      "source": [
        "## Pre-process raw draft data in chunks, and save into smaller files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n36BniXik9gs"
      },
      "outputs": [],
      "source": [
        "filename = f'draft_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "datatypes = get_dtypes(filename)\n",
        "\n",
        "# Get the length of full dataset\n",
        "dataset_length = len(pd.read_csv(filename, dtype=datatypes, compression='gzip', index_col=False, usecols=[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "BS4dEUHlKpPW",
        "outputId": "dc7254b9-ef70-473f-f59e-4673ea981d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Data...\n",
            "Preprocessing Complete\n",
            "Chunk 0 complete in 179 seconds.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4cb4bb11-8e0e-4926-9ecc-12ca58d55b46\", \"NEO_preprocessed_0.parquet.gz\", 3768153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Number of rows to per chunk\n",
        "num_rows = 250000\n",
        "num_chunks = dataset_length // num_rows\n",
        "\n",
        "for chunk in range(num_chunks):\n",
        "    t0 = time.time()\n",
        "    # Load batch of rows from raw data file\n",
        "    end_idx = (chunk * num_rows)\n",
        "    data_chunk = pd.read_csv(filename, dtype=datatypes, compression='gzip',\n",
        "                             nrows=num_rows, header=0, skiprows=range(1, end_idx+1))\n",
        "    # Preprocess the batch\n",
        "    processed = DataPreprocess(data_chunk, set_name=SET_NAME)\n",
        "    processed.initialize()\n",
        "    processed.process_data()\n",
        "\n",
        "    # Save batch to file\n",
        "    f_name = f'{SET_NAME}_preprocessed_{chunk}.parquet.gz'\n",
        "    processed.df_cleaned.to_parquet(f_name, compression='gzip')\n",
        "    t1 = round(time.time() - t0)\n",
        "    print(f'Chunk {chunk} complete in {t1} seconds.')\n",
        "    files.download(f_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSW738ERCSrQ"
      },
      "source": [
        "## Re-combine smaller files into final pre-processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQxk8j_eBIi2"
      },
      "outputs": [],
      "source": [
        "# Load first file\n",
        "pq_file = f'{SET_NAME}_preprocessed_0.parquet.gz'\n",
        "pq_0 = pd.read_parquet(pq_file)\n",
        "\n",
        "# Then iteratively append the rest of the files\n",
        "for chunk in range(1 ,num_chunks):\n",
        "  pq_next_file = f'{SET_NAME}_preprocessed_{chunk}.parquet.gz'\n",
        "  pq_next = pd.read_parquet(pq_next_file)\n",
        "  pq_0 = pd.concat([pq_0,pq_next])\n",
        "\n",
        "# Save reconstructed dataset\n",
        "pq_file_out = f'{SET_NAME}_all.parquet.gz'\n",
        "pq_0.to_parquet(pq_file_out, compression='gzip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmPz2w3bEIFu"
      },
      "source": [
        "* LTR draft length: 6,751,107 rows\n",
        "* SIR draft length: 2,174,413 rows (all processed)\n",
        "* MKM draft length: 6,180,337 rows (all processed)\n",
        "* NEO draft length: 4,732,717 rows (all processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7baopZVOwUpq"
      },
      "source": [
        "## Correct wins totals if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZId2DCq6nzej",
        "outputId": "509621a0-8040-4061-9c36-a15cb654cc6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "game_data_public.MKM.PremierDraft.csv.gz already exists, no need to download.\n"
          ]
        }
      ],
      "source": [
        "filename = f'{SET_NAME}_all.parquet.gz'\n",
        "out_file = f'{SET_NAME}_corrected.parquet.gz'\n",
        "\n",
        "df = pd.read_parquet(filename)\n",
        "game_wins = get_game_wins(SET_NAME)\n",
        "df = map_game_wins(df, game_wins)\n",
        "df.to_parquet(out_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "SET_NAME = 'SIR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Data...\n",
            "Preprocessing Complete\n"
          ]
        }
      ],
      "source": [
        "# Import prepared card feature and oracle text files\n",
        "card_features = pd.read_parquet('card_features_reduced.parquet')\n",
        "oracle_texts = pd.read_parquet('oracle_text.parquet')\n",
        "\n",
        "# Extract Set card list from DataPreprocessing Object\n",
        "# filename = f'draft_data_public.{SET_NAME}.PremierDraft.csv.gz'\n",
        "filename = 'draft_data_SIR\\SIR_premier_draft_1.csv'\n",
        "datatypes = get_dtypes(filename)\n",
        "data = pd.read_csv(filename, dtype=datatypes)\n",
        "\n",
        "\n",
        "data_header = pd.read_csv(filename, dtype=datatypes, nrows=0)\n",
        "card_head = DataPreprocess(data_header, set_name=SET_NAME)\n",
        "card_head.initialize()\n",
        "cards_df = card_head.set_cards\n",
        "\n",
        "# Merge Set card list with imported features\n",
        "cards_df = cards_df.merge(oracle_texts, left_on='name', right_on='name').drop(columns=['arena_id'], axis=1)\n",
        "cards_df = cards_df.merge(card_features, left_on='name', right_on='name')\n",
        "\n",
        "processed = DataPreprocess(data, set_name=SET_NAME)\n",
        "processed.initialize()\n",
        "processed.process_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>token_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>Travel Preparations</td>\n",
              "      <td>358</td>\n",
              "      <td>86670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    name  token_id     id\n",
              "356  Travel Preparations       358  86670"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed.vocab_size\n",
        "processed.set_cards[processed.set_cards['token_id']==358]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>token_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Rattlechains</td>\n",
              "      <td>40</td>\n",
              "      <td>72228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            name  token_id     id\n",
              "38  Rattlechains        40  72228"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Data...\n",
            "Preprocessing Complete\n"
          ]
        }
      ],
      "source": [
        "processed = DataPreprocess(data, set_name=SET_NAME)\n",
        "processed.initialize()\n",
        "processed.process_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          
        }
      ],
      "source": [
        "processed.df_cleaned.to_parquet('SIR_no_pad.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([362,  40,  96,  39], dtype=int64)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df['pool_with_wins'] \n",
        "\n",
        "vocab_size = processed.set_cards['token_id'].max()\n",
        "\n",
        "a = df['cards_in_pool'][3]\n",
        "a_len = df['cards_in_pool'][3].shape[0]\n",
        "\n",
        "np.full_like(df['cards_in_pool'][3], df['wins'][3])\n",
        "\n",
        "new_a = np.full(a_len + 1, (vocab_size + 1 + df['wins'][3]))\n",
        "new_a[1:] = a\n",
        "new_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([list([317, 212, 276, 83, 286, 36, 86, 22, 225, 132, 24, 198, 40, 173, 242]),\n",
              "       list([321, 264, 186, 296, 154, 188, 127, 71, 24, 229, 96, 6, 203, 53]),\n",
              "       list([214, 119, 21, 191, 196, 62, 356, 165, 39, 33, 102, 239, 142]),\n",
              "       ..., list([157, 160, 267, 26]), list([46, 239, 274]),\n",
              "       list([29, 102])], dtype=object)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['cards_in_pack'].to_numpy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
